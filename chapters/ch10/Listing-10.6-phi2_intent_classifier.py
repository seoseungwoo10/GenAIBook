# ëª¨ë“ˆ ì„¤ëª…: Listing 10.6 - Phi-2 ëª¨ë¸ì„ ì‚¬ìš©í•œ ì˜ë„ ë¶„ë¥˜(Intent Classification)
# - Microsoftì˜ ì†Œí˜• ì–¸ì–´ ëª¨ë¸ Phi-2ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
# - ê°•ì•„ì§€ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ íŒë³„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ê²½ë¡œë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.
# - ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì€ ëª¨ë¸ë¡œ ë¹„ìš© ì ˆê° ë° ë ˆì´í„´ì‹œ ê°ì†Œ
#
# ì£¼ìš” ê°œë…:
# - Phi-2: Microsoftì˜ 2.7B íŒŒë¼ë¯¸í„° ì†Œí˜• LLM (íš¨ìœ¨ì ì´ì§€ë§Œ ì„±ëŠ¥ ìš°ìˆ˜)
# - Intent Classification: ì‚¬ìš©ì ì…ë ¥ì˜ ì˜ë„ë¥¼ íŒŒì•…í•˜ëŠ” NLP íƒœìŠ¤í¬
# - Model Routing: ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸/ì²˜ë¦¬ ê²½ë¡œ ì„ íƒ
# - Transformers: Hugging Faceì˜ ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  ë¼ì´ë¸ŒëŸ¬ë¦¬

# We need to ensure the following packages are installed:
# pip install transformers==4.42.4 torch==2.3.1

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import warnings
import re
import logging

DEBUG = True

warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)

# Phi-2 ëª¨ë¸ ë¡œë“œ
# torch_dtype="auto": ìë™ìœ¼ë¡œ ìµœì  ë°ì´í„° íƒ€ì… ì„ íƒ (GPU ê°€ìš©ì‹œ float16)
# trust_remote_code=True: ì»¤ìŠ¤í…€ ëª¨ë¸ ì½”ë“œ ì‹¤í–‰ í—ˆìš©
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    torch_dtype="auto",
    trust_remote_code=True
)

# í† í¬ë‚˜ì´ì € ë¡œë“œ (í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜)
tokenizer = AutoTokenizer.from_pretrained(
    "microsoft/phi-2",
    trust_remote_code=True
)

# Set the default device to CUDA if available, otherwise use CPU
# GPUê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ CPU ì‚¬ìš©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Check if the question is about dogs
# ì§ˆë¬¸ì´ ê°•ì•„ì§€ì— ê´€í•œ ê²ƒì¸ì§€ íŒë³„í•˜ëŠ” í•¨ìˆ˜
def check_dog_question(question):
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ê°•ì•„ì§€ ê´€ë ¨ ì—¬ë¶€ë¥¼ yes/noë¡œ ë‹µë³€í•˜ë„ë¡ ì§€ì‹œ
    system_prompt = f"Instruct: Is there anything about dogs in the question below? If yes, answer with 'yes' else 'no'.\nQuestion:{question}\nOutput:"
    prompt = f"{system_prompt}\nUser:{question}\nOutput:"
    
    # ì¶”ë¡  ì‹¤í–‰ (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¶ˆí•„ìš”)
    with torch.no_grad():
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            return_attention_mask=False,
            add_special_tokens=False
        )
        if DEBUG:
            print(f"Calling model with Inputs:{inputs}")

        # ì…ë ¥ì„ ëª¨ë¸ì´ ìˆëŠ” ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}

        # í…ìŠ¤íŠ¸ ìƒì„± (ìµœëŒ€ 500í† í°)
        outputs = model.generate(
            **inputs,
            max_length=500,
            pad_token_id=tokenizer.eos_token_id
        )

    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    text = tokenizer.batch_decode(outputs)[0]

    # Remove the prompt from the output text
    # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±° (ë‹µë³€ë§Œ ë‚¨ê¸°ê¸°)
    text = text.replace(prompt, '').strip()
    text = text.replace("<|endoftext|>", '').strip()
    
    if DEBUG:
        print(f"Answer:{text}")

    # ì •ê·œì‹ìœ¼ë¡œ "Output: Yes" íŒ¨í„´ ì°¾ê¸°
    regex = "^Output: Yes$"
    match = re.search(regex, text, re.MULTILINE)
    if match:
        if DEBUG:
            print("Found a match:", match.group())
        return True
    else:
        if DEBUG:
            print("No match found")
    
    return False

# Handle the user prompt
# ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì‘ë‹µ ìƒì„±
def handle_prompt(user_input)->str:
    prompt = f"Instruct: Tell me more about this:{user_input}\nOutput:"

    with torch.no_grad():
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            return_attention_mask=False,
            add_special_tokens=False
        )
        inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}
        outputs = model.generate(
            **inputs,
            max_length=500,
            pad_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.batch_decode(outputs)[0]

    # Remove the prompt from the output text
    text = text.replace(prompt, '').strip()
    text = text.replace("<|endoftext|>", '').strip()
    
    return text

# Handle the dog question
# ê°•ì•„ì§€ ê´€ë ¨ ì§ˆë¬¸ì„ ì²˜ë¦¬ (RAG + GPT-4 í˜¸ì¶œ ê°€ëŠ¥)
def handle_dog_question(question):
    # Handle the question using RAG and GPT4
    # This is a placeholder function, to show a proxy; we don't actually call the OpenAI model.
    # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ RAG ì‹œìŠ¤í…œ + GPT-4ë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±

    # Call OpenAI's GPT-4 to answer the question
    # Implement openai call here
    openai_response = f"This is a proxy to show that you are calling OpenAI's GPT-4 to answer the question: {question}"

    # ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ:
    # openai.api_key = "your-openai-api-key"
    # openai_response = openai.Completion.create(
    #   engine="gpt-4",
    #   prompt=f"Ask a question about dogs: {question}",
    #   max_tokens=400
    # )
    
    return openai_response
    
# Main function
if __name__=="__main__": 
    # Loop until the user enters "quit"
    while True:
        # Take user input
        user_prompt = input("What is your question (or type 'quit' to exit):")

        if user_prompt.casefold() == 'quit':
            break

        # ì˜ë„ ë¶„ë¥˜: ê°•ì•„ì§€ ê´€ë ¨ ì—¬ë¶€ íŒë³„
        if check_dog_question(user_prompt):
            # ê°•ì•„ì§€ ê´€ë ¨ ì§ˆë¬¸ -> ì „ë¬¸ RAG ì‹œìŠ¤í…œ ì‚¬ìš©
            print(handle_dog_question(user_prompt))
        else:
            # ì¼ë°˜ ì§ˆë¬¸ -> ê°„ë‹¨í•œ ì‘ë‹µ
            print("ğŸ¤– You did not ask about dogs")
            print("handle_prompt(user_prompt)")
    print("-" * 100)